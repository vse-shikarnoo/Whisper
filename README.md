# Audio Transcription & Summarization API

Local application for audio/video file transcription with speaker identification and intelligent text summarization.

## üåü Features

- üé§ **Transcription** of audio and video files (Whisper)
- üë• **Speaker identification** (diarization)
- üìù **Full text** without segmentation
- ü§ñ **Intelligent summarization** via Ollama
- üîÑ **Fallback summarization** via transformers
- üìä **Detailed segments** with timestamps
- üöÄ **Fully local operation**
- üîß **REST API** for integration

## üõ† Technologies

- **Whisper** - audio transcription
- **PyAnnote** - speaker diarization
- **Ollama** - intelligent summarization
- **Transformers** - fallback summarization
- **FastAPI** - web interface
- **PyTorch** - machine learning

## üì¶ Installation

### 1. Clone Repository

```bash
git clone <repository-url>
cd audio-transcription-api
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Install Ollama (Optional)

**Windows**: Download from [ollama.com](https://ollama.com/)

**Linux/Mac**:
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 4. Environment Variables

```bash
# For diarization (optional)
export HF_TOKEN="your_huggingface_token"

# For Ollama (optional)
export OLLAMA_MODEL="llama3.2"
export OLLAMA_HOST="http://localhost:11434"
```

## üöÄ Quick Start

```bash
python main.py
```

Server will start at `http://localhost:8000`

## üìö API Documentation

### Main Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/process` | Process audio/video file |
| `GET` | `/results/{id}` | Get result by ID |
| `GET` | `/download/{id}` | Download result as JSON |
| `GET` | `/health` | API health check |
| `GET` | `/status` | Detailed system status |
| `GET` | `/ollama/models` | List Ollama models |

### File Processing

#### Basic Request

```bash
curl -X POST "http://localhost:8000/process" \
     -F "file=@audio.mp3"
```

#### With Parameters

```bash
# With Ollama (recommended)
curl -X POST "http://localhost:8000/process?use_ollama=true" \
     -F "file=@audio.mp3"

# Without Ollama (uses transformers)
curl -X POST "http://localhost:8000/process?use_ollama=false" \
     -F "file=@audio.mp3"

# Specific Ollama model
curl -X POST "http://localhost:8000/process?use_ollama=true&ollama_model=mistral" \
     -F "file=@audio.mp3"
```

### Getting Results

```bash
# Get result by ID
curl "http://localhost:8000/results/12345678-1234-1234-1234-123456789abc"

# Download as file
curl -o transcription.json \
     "http://localhost:8000/download/12345678-1234-1234-1234-123456789abc"
```

### Ollama Management

```bash
# Get available models
curl "http://localhost:8000/ollama/models"

# Change Ollama model
curl -X POST "http://localhost:8000/ollama/set_model?model=llama3.2"
```

## üìã Supported Formats

### Audio
- MP3, WAV, M4A, FLAC

### Video
- MP4, AVI, MOV, MKV

## üìä Response Structure

```json
{
  "result_id": "uuid",
  "full_text": "Complete transcription text without segmentation...",
  "full_text_summary": "Summary of the entire text...",
  "segments": [
    {
      "start": 0.0,
      "end": 5.2,
      "text": "Segment text",
      "speaker": "speaker_1"
    }
  ],
  "speaker_texts": {
    "speaker_1": "All text from speaker 1...",
    "speaker_2": "All text from speaker 2..."
  },
  "speaker_summaries": {
    "speaker_1": "Summary of speaker 1's speech...",
    "speaker_2": "Summary of speaker 2's speech..."
  },
  "speakers_count": 2,
  "total_segments": 15,
  "total_text_length": 2450,
  "ollama_enabled": true,
  "ollama_model": "llama3.2",
  "summarization_method": "ollama",
  "processing_quality": "high"
}
```

## üéØ Usage Examples

### Python Client

```python
import requests

def process_audio(file_path):
    with open(file_path, 'rb') as f:
        files = {'file': (file_path, f, 'audio/mpeg')}
        response = requests.post(
            'http://localhost:8000/process?use_ollama=true',
            files=files
        )
    
    if response.status_code == 200:
        result = response.json()
        print(f"Processing complete! ID: {result['result_id']}")
        print(f"Speakers: {result['speakers_count']}")
        print(f"Summary: {result['full_text_summary']}")
        return result
    else:
        print(f"Error: {response.text}")
        return None

# Usage
result = process_audio('meeting.mp3')
```

### Bash Script

```bash
#!/bin/bash
process_audio() {
    local file=$1
    local response=$(curl -s -X POST "http://localhost:8000/process?use_ollama=true" -F "file=@$file")
    local result_id=$(echo $response | grep -o '"result_id":"[^"]*"' | cut -d'"' -f4)
    
    if [ -n "$result_id" ]; then
        echo "Processing complete. ID: $result_id"
        curl -s "http://localhost:8000/results/$result_id" | jq '.full_text_summary'
    else
        echo "Processing error"
        echo $response
    fi
}

process_audio $1
```

## ‚öôÔ∏è Model Configuration

### Whisper Models
- `tiny` - fast, low quality
- `base` - balanced speed/quality
- `small` - good quality
- `medium` - high quality (recommended)
- `large` - best quality, slow

Change in code:
```python
self.whisper_model = whisper.load_model("medium")
```

### Ollama Models
```bash
# Install models
ollama pull llama3.2
ollama pull mistral
ollama pull codellama

# Check installed models
ollama list
```

## üîß Troubleshooting

### Diarization Issues
```bash
# If no HF_TOKEN, diarization will be disabled
export HF_TOKEN="your_huggingface_token"
```

### Ollama Issues
```bash
# Check Ollama status
ollama serve

# Ensure models are loaded
curl http://localhost:11434/api/tags
```

### Dependency Issues
```bash
# Reinstall PyTorch (for GPU)
pip uninstall torch torchaudio
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118

# Install FFmpeg
# Ubuntu/Debian
sudo apt install ffmpeg

# Windows: download from ffmpeg.org
```

## üìà Performance

| Component | Resources | Processing Time (1 min audio) |
|-----------|-----------|-------------------------------|
| Whisper Base | 1GB RAM | ~30 seconds |
| Whisper Medium | 2GB RAM | ~60 seconds |
| Diarization | 2GB RAM | ~45 seconds |
| Ollama Summarization | 4GB RAM | ~15 seconds |

## ü§ù Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## üìÑ License

This project is licensed under the MIT License. See `LICENSE` file for details.

## üìû Support

If you encounter issues:

1. Check all dependencies are installed
2. Ensure Ollama is running (`ollama serve`)
3. Check application logs
4. Create an issue in the repository


# Audio Transcription & Summarization API

–õ–æ–∫–∞–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ —Ñ–∞–π–ª–æ–≤ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Å–ø–∏–∫–µ—Ä–æ–≤ –∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–∞.

## üåü –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- üé§ **–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è** –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ —Ñ–∞–π–ª–æ–≤ (Whisper)
- üë• **–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤** (–¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è) 
- üìù **–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç** –±–µ–∑ —Ä–∞–∑–±–∏–≤–∫–∏ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º
- ü§ñ **–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è** —á–µ—Ä–µ–∑ Ollama
- üîÑ **–†–µ–∑–µ—Ä–≤–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è** —á–µ—Ä–µ–∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã
- üìä **–î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã** —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
- üöÄ **–ü–æ–ª–Ω–æ—Å—Ç—å—é –ª–æ–∫–∞–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞**
- üîß **REST API** –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏

## üõ† –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏

- **Whisper** - —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –∞—É–¥–∏–æ
- **PyAnnote** - –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤)
- **Ollama** - –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
- **Transformers** - —Ä–µ–∑–µ—Ä–≤–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
- **FastAPI** - –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
- **PyTorch** - –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

## üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞

### 1. –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è

```bash
git clone <repository-url>
cd audio-transcription-api
```

### 2. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
pip install -r requirements.txt
```

### 3. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

**Windows**: –°–∫–∞—á–∞–π—Ç–µ —Å [ollama.com](https://ollama.com/)

**Linux/Mac**:
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 4. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è

```bash
# –î–ª—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
export HF_TOKEN="your_huggingface_token"

# –î–ª—è Ollama (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
export OLLAMA_MODEL="llama3.2"
export OLLAMA_HOST="http://localhost:11434"
```

## üöÄ –ó–∞–ø—É—Å–∫

```bash
python main.py
```

–°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—Å—Ç–∏—Ç—Å—è –Ω–∞ `http://localhost:8000`

## üìö API –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

### –û—Å–Ω–æ–≤–Ω—ã–µ endpoint'—ã

| –ú–µ—Ç–æ–¥ | Endpoint | –û–ø–∏—Å–∞–Ω–∏–µ |
|-------|----------|----------|
| `POST` | `/process` | –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ —Ñ–∞–π–ª–∞ |
| `GET` | `/results/{id}` | –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–æ ID |
| `GET` | `/download/{id}` | –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∫–∞–∫ JSON |
| `GET` | `/health` | –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ API |
| `GET` | `/status` | –ü–æ–¥—Ä–æ–±–Ω—ã–π —Å—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã |
| `GET` | `/ollama/models` | –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π Ollama |

### –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤

#### –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å

```bash
curl -X POST "http://localhost:8000/process" \
     -F "file=@audio.mp3"
```

#### –° –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏

```bash
# –° –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Ollama (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
curl -X POST "http://localhost:8000/process?use_ollama=true" \
     -F "file=@audio.mp3"

# –ë–µ–∑ Ollama (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã)
curl -X POST "http://localhost:8000/process?use_ollama=false" \
     -F "file=@audio.mp3"

# –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è –º–æ–¥–µ–ª—å Ollama
curl -X POST "http://localhost:8000/process?use_ollama=true&ollama_model=mistral" \
     -F "file=@audio.mp3"
```

### –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

```bash
# –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ ID
curl "http://localhost:8000/results/12345678-1234-1234-1234-123456789abc"

# –°–∫–∞—á–∞—Ç—å –∫–∞–∫ —Ñ–∞–π–ª
curl -o transcription.json \
     "http://localhost:8000/download/12345678-1234-1234-1234-123456789abc"
```

### –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ Ollama

```bash
# –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
curl "http://localhost:8000/ollama/models"

# –°–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å Ollama
curl -X POST "http://localhost:8000/ollama/set_model?model=llama3.2"
```

## üìã –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã

### –ê—É–¥–∏–æ
- MP3, WAV, M4A, FLAC

### –í–∏–¥–µ–æ  
- MP4, AVI, MOV, MKV

## üìä –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞

```json
{
  "result_id": "uuid",
  "full_text": "–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –±–µ–∑ —Ä–∞–∑–±–∏–≤–∫–∏...",
  "full_text_summary": "–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –≤—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞...",
  "segments": [
    {
      "start": 0.0,
      "end": 5.2,
      "text": "–¢–µ–∫—Å—Ç —Å–µ–≥–º–µ–Ω—Ç–∞",
      "speaker": "speaker_1"
    }
  ],
  "speaker_texts": {
    "speaker_1": "–í–µ—Å—å —Ç–µ–∫—Å—Ç –ø–µ—Ä–≤–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞...",
    "speaker_2": "–í–µ—Å—å —Ç–µ–∫—Å—Ç –≤—Ç–æ—Ä–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞..."
  },
  "speaker_summaries": {
    "speaker_1": "–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Ä–µ—á–∏ —Å–ø–∏–∫–µ—Ä–∞ 1...",
    "speaker_2": "–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Ä–µ—á–∏ —Å–ø–∏–∫–µ—Ä–∞ 2..."
  },
  "speakers_count": 2,
  "total_segments": 15,
  "total_text_length": 2450,
  "ollama_enabled": true,
  "ollama_model": "llama3.2",
  "summarization_method": "ollama",
  "processing_quality": "high"
}
```

## üéØ –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### Python –∫–ª–∏–µ–Ω—Ç

```python
import requests

def process_audio(file_path):
    with open(file_path, 'rb') as f:
        files = {'file': (file_path, f, 'audio/mpeg')}
        response = requests.post(
            'http://localhost:8000/process?use_ollama=true',
            files=files
        )
    
    if response.status_code == 200:
        result = response.json()
        print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! ID: {result['result_id']}")
        print(f"–°–ø–∏–∫–µ—Ä–æ–≤: {result['speakers_count']}")
        print(f"–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {result['full_text_summary']}")
        return result
    else:
        print(f"–û—à–∏–±–∫–∞: {response.text}")
        return None

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
result = process_audio('meeting.mp3')
```

### Bash —Å–∫—Ä–∏–ø—Ç

```bash
#!/bin/bash
process_audio() {
    local file=$1
    local response=$(curl -s -X POST "http://localhost:8000/process?use_ollama=true" -F "file=@$file")
    local result_id=$(echo $response | grep -o '"result_id":"[^"]*"' | cut -d'"' -f4)
    
    if [ -n "$result_id" ]; then
        echo "–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. ID: $result_id"
        curl -s "http://localhost:8000/results/$result_id" | jq '.full_text_summary'
    else
        echo "–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏"
        echo $response
    fi
}

process_audio $1
```

## ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–µ–π

### –ú–æ–¥–µ–ª–∏ Whisper
- `tiny` - –±—ã—Å—Ç—Ä–∞—è, –Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
- `base` - –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞
- `small` - —Ö–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
- `medium` - –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
- `large` - –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –º–µ–¥–ª–µ–Ω–Ω–∞—è

–ò–∑–º–µ–Ω–∏—Ç–µ –≤ –∫–æ–¥–µ:
```python
self.whisper_model = whisper.load_model("medium")
```

### –ú–æ–¥–µ–ª–∏ Ollama
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π
ollama pull llama3.2
ollama pull mistral
ollama pull codellama

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
ollama list
```

## üîß –†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º

### –ü—Ä–æ–±–ª–µ–º—ã —Å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π
```bash
# –ï—Å–ª–∏ –Ω–µ—Ç HF_TOKEN, –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç –æ—Ç–∫–ª—é—á–µ–Ω–∞
export HF_TOKEN="your_huggingface_token"
```

### –ü—Ä–æ–±–ª–µ–º—ã —Å Ollama
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å Ollama
ollama serve

# –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã
curl http://localhost:11434/api/tags
```

### –ü—Ä–æ–±–ª–µ–º—ã —Å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
```bash
# –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch (–¥–ª—è GPU)
pip uninstall torch torchaudio
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ FFmpeg
# Ubuntu/Debian
sudo apt install ffmpeg

# Windows: —Å–∫–∞—á–∞–π—Ç–µ —Å ffmpeg.org
```

## üìà –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –†–µ—Å—É—Ä—Å—ã | –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (1 –º–∏–Ω –∞—É–¥–∏–æ) |
|-----------|---------|-------------------------------|
| Whisper Base | 1GB RAM | ~30 —Å–µ–∫—É–Ω–¥ |
| Whisper Medium | 2GB RAM | ~60 —Å–µ–∫—É–Ω–¥ |
| –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è | 2GB RAM | ~45 —Å–µ–∫—É–Ω–¥ |
| Ollama —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è | 4GB RAM | ~15 —Å–µ–∫—É–Ω–¥ |

## ü§ù –í–∫–ª–∞–¥ –≤ –ø—Ä–æ–µ–∫—Ç

1. –§–æ—Ä–∫–Ω–∏—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
2. –°–æ–∑–¥–∞–π—Ç–µ –≤–µ—Ç–∫—É –¥–ª—è —Ñ–∏—á–∏ (`git checkout -b feature/amazing-feature`)
3. –ó–∞–∫–æ–º–º–∏—Ç—å—Ç–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è (`git commit -m 'Add amazing feature'`)
4. –ó–∞–ø—É—à—å—Ç–µ –≤ –≤–µ—Ç–∫—É (`git push origin feature/amazing-feature`)
5. –û—Ç–∫—Ä–æ–π—Ç–µ Pull Request

## üìÑ –õ–∏—Ü–µ–Ω–∑–∏—è

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç—Å—è –ø–æ–¥ –ª–∏—Ü–µ–Ω–∑–∏–µ–π MIT. –°–º. —Ñ–∞–π–ª `LICENSE` –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–µ–π.

## üìû –ü–æ–¥–¥–µ—Ä–∂–∫–∞

–ï—Å–ª–∏ —É –≤–∞—Å –≤–æ–∑–Ω–∏–∫–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã:

1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
2. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω (`ollama serve`)
3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
4. –°–æ–∑–¥–∞–π—Ç–µ issue –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏

---

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ**: –î–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ Ollama —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.
